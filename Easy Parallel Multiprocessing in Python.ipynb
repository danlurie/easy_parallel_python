{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Easy Parallel Multiprocessing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With less than 10 extra lines of code, you can take your existing code and utilize all those extra cores on your computer.\n",
    "\n",
    "Let's explore how this is done using a toy example.\n",
    "\n",
    "### Toy Example: SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple function to do a Singular Value Decomposition on an input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_svd(in_array):\n",
    "    return np.linalg.svd(in_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20 random 1000 by 1000 arrays. This is our \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array_list = []\n",
    "for i in xrange(20):\n",
    "    array_list.append(np.random.rand(1000,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through each of the 20 arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 963 ms, total: 1min 11s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svd_out_loop = []\n",
    "for i in array_list:\n",
    "    svd_out_loop.append(run_svd(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A whole minute!? I don't know about you, but I get bored pretty quickly. Let's see if we can speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a multiprocessing pool with 10 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool = Pool(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looping through items, we will submit them to our multiprocesing pool, which will map our `run_svd` function across the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.21 s, sys: 869 ms, total: 2.08 s\n",
      "Wall time: 9.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svd_out_map = pool.map(run_svd, array_list)\n",
    "pool.close() # Close the pool when processing is finished.\n",
    "pool.join() # Collect the results from the multiple parallel processing threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better! Now let's look at a real-world example.\n",
    "\n",
    "### Practical Example: Smoothing fMRI data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I have written a function to run spatial smoothing on fMRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_smoothing(sub_id, file_path_template, fwhm, out_dir):\n",
    "    \"\"\"\n",
    "    Run spatial smoothing using AFNI 3dBlurToFWHM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sub_id : str\n",
    "        Unique subject identifier.\n",
    "    file_path_template : str\n",
    "        Path to input files with with sub_id to be inserted via string formatting.\n",
    "    fwhm : int\n",
    "        FWHM of the 3D Gaussian kernel to use for smoothing.\n",
    "    out_dir : str\n",
    "        Path to an existing directory where smoothed files should be written.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        func_path = file_path_template.format(sub_id)\n",
    "        fname = '{}_4mm_fwhm_{}.nii.gz'.format(sub_id, str(fwhm))\n",
    "        out_path = '/'.join([out_dir, fname])\n",
    "        shell_out = subprocess.check_output(['3dBlurToFWHM', '-input', func_path, '-FWHM', fwhm, \n",
    "                                             '-prefix', out_path]) \n",
    "    except:\n",
    "        return 'Error encountered during smoothing for subject %s.' % sub_id\n",
    "        pass\n",
    "    else:\n",
    "        return 'Smoothing complete for %s.' % sub_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! My `run_smoothing` function has **4 required arguments, but `Pool.map` can only pass a single argument** to the function we're mapping.\n",
    "\n",
    "In this case, only one of the required arguments for our function changes as we iterate (`sub_id`); all other arguments stay constant. \n",
    "\n",
    "We could hard-code the other arguments inside the function, but that's clunky. There has to be another way.\n",
    "\n",
    "### `functools.partial` to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `functools.partial`, we can create a \"partial\" function from `run_smoothing` for which all but one of the input arguments are pre-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp_template = '/path/to/fmri_data/{}_preprocessed.nii.gz'\n",
    "kernel_width = 6\n",
    "o_dir = '/path/to/smoothed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_smoothing_partial = partial(run_smoothing, file_path_template=fp_template, fwhm=kernel_width, out_dir=o_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then map our new function across the subject list, just as we did in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject_list = np.loadtxt('/path/to/subject_list.txt', dtype='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = Pool(10)\n",
    "smoothing_out = pool.map(run_smoothing_partial, sub_list)\n",
    "pool.close() \n",
    "pool.join() # In this case, we don't actually return anything from the function, but we join the output anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `multiprocessing.Pool` function is by far the easiest way to parallellize simple code, but it has some issues:\n",
    "- It doesn't always play well with iPython or other interactive interpreters.\n",
    "- It spawns a full new Python proess for each thread in the pool. This can add un-necessary overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative: `IPython.parallel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly, here is how the equivalent functionality from the toy example would be achieved using `IPython.parallel`.\n",
    "\n",
    "First, start a new IPython Notebook server. Click the \"Clusters\" tab, and start a new cluster with your desired number of engines (cores).\n",
    "\n",
    "Then, in a new Notebook page, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your local parallel cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the engines in our cluster.\n",
    "clients = parallel.Client()\n",
    "# Tell each engine to return all results at once.\n",
    "clients.block = True\n",
    "# Get a direct view of the cluster.\n",
    "dview = clients.direct_view()\n",
    "# Get a load-balanced view of the cluster.\n",
    "lbview = clients.load_balanced_view()\n",
    "# Set our views to run return all results at once.\n",
    "dview.block = True\n",
    "lbview.block = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your function. Note that you have to do any imports **within the function** and explicitly push the new function to the nodes in your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_svd(in_array):\n",
    "    import numpy as np\n",
    "    return np.linalg.svd(in_array)\n",
    "# Push to all cluster engines.\n",
    "clients[:].push(dict(run_svd=run_svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map the array list across cluster engines.\n",
    "svd_out_ipypool = lbview.map(run_svd, array_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `joblib`: the easiest way to parallelize your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only recently discovered [joblib](https://pythonhosted.org/joblib/index.html), but it has become my go-to method when I need to run things in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of `joblib` is that it wraps `multiprocessing.Pool` for you, so you don't have to manually create partial functions.\n",
    "\n",
    "Instead, you pass your function and arguments to `joblib.Parallel`, and use list comprehension to define the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jl_report = Parallel(n_jobs=15, backend='threading')(\n",
    "    delayed(run_smoothing)(sub_id, file_path_template, fwhm, out_dir)\n",
    "    for sub_id in subject_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above uses threading, which in some cases can be more efficient than forking the job into multiple distinct processes, which requires sending data back-and-forth between processes. The [Embarrasingly parallel for loops](https://pythonhosted.org/joblib/parallel.html) page on the joblib website has more detail about how to use `joblib.Parallel` and when to use threading vs. forking. That page also discusses how to use pre-allocated memory-maps to work with data that is too large to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Multiprocessing in Python: A Guided Tour](http://www.davekuhlman.org/python_multiprocessing_01.html)\n",
    "- [Easily distributing a parallel IPython Notebook on a cluster](http://twiecki.github.io/blog/2014/02/24/ipython-nb-cluster/)\n",
    "- [Using IPython for parallel computing](http://ipython.org/ipython-doc/2/parallel/index.html)\n",
    "- [An introduction to parallel programming using Python's multiprocessing module](http://sebastianraschka.com/Articles/2014_multiprocessing_intro.html)\n",
    "- [Parallelism in One Line](http://chriskiehl.com/article/parallelism-in-one-line/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
